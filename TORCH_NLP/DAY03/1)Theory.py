# 범주형 Data
## 순서형 -> 정수형 (ex 랭킹, 벌점...) -> 정수인코딩
## 명목형 (ex 성별, 혈액형, 지역명...)  -> Ond-Hot 인코딩

## 우리가 자연어에서 정수로 변환해준것도 순서가 중요하지않음 -> One-Hot 인코딩 필요
## 문장 길이 토큰이 50이고, 단어 사전이 100개인 경우의 shape -> (50, 100)
## 단어사전에서 One-Hot 인코딩을 해줬을 때 
## ex
## 00100000000000....0000
## 00010000000000....0000
## 00001000000000....0000
## 00000100000000....0000
##      ....
##      ....
## 00000000000000....0001
## => 문장길이(행의 수= 토큰의 수)가 50이고, 단어사전이 100개 => shape (50, 100)
### > 0이 많으면 차원의 저주, 차원의 저주를 줄이고자 임베딩 사용
## ==> 더 줄이기 위해서 임베딩(Embedding) 사용 
## 임베딩(Embedding) : 100인데 10으로 줄이는거 (줄이는 건 개발자 마음대로)
## 참고: p232 ~ 

## ex 오늘은 좋은날!, 오늘 영화 보러 가자, 오는 길에 빵을 사와라
## -> 토큰화 (중복되는 건 1번만)
## p234 
## 오, 늘, 은, 좋, 날, 영, 화, 보, 러, 가, 자, 는, 길, 에, 빵, 을, 사, 와, 라  => 완성형 
## ㅇ, ㅗ, ㄴ, ㅡ, ㄹ, ...                                                   => 조합형 (조합에서 글자를 만들때마다 인코딩 진행)
## 자모(jamo) 라이브러리를 통해서 자음, 모음으로 분리해서 진행
## jamo 라이브러리 https://pypi.org/project/jamo/

## 한글을 자모로 바꿔주는 라이브러리 
# >>> from jamo import h2j
# >>> h2j("한굴")
# '한굴'

# >>> from jamo import h2j, j2hcj
# >>> j2hcj(h2j("한굴"))
# 'ㅎㅏㄴㄱㅜㄹ'
# >>> j2hcj(h2j("자모=字母=jamo"))
# 'ㅈㅏㅁㅗ=字母=jamo'

# >>> from jamo import hangul_to_jamo
# >>> long_story = open("구운몽.txt", 'r').read()
# >>> hangul_to_jamo(long_story)
# <generator object <genexpr> at 0xdeadbeef9001>
# # -> 용량이 커서 generator로 반환

### ===> 하는 이유가 OOV 때문 

# 자음 ㄱ, ㄴ, ㄷ, ....., ㅎ 
# 모음 ㅏ, ㅑ, ......, ㅣ 
# 가장 많이 사용하는 형태소 분석기 KoNLPy