{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p324 ~ p335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from tabulate import tabulate\n",
    "\n",
    "# 명령 프롬프트에서 가상환경 활성화 후 conda install tabulate 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data세트 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\hwans\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\hwans\\Korpora\\nsmc\\ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size :  45000\n",
      "Testing Data Size :  5000\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)\n",
    "\n",
    "train = corpus_df.sample(frac=0.9, random_state = 42)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size : \", len(train))\n",
    "print(\"Testing Data Size : \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data 토큰화 및 단어사전 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens) :\n",
    "    counter = Counter()\n",
    "    for tokens in corpus :\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab) :\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab = 5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
    "token_to_id = {token : idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx : token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정수 인코딩 및 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value) :\n",
    "    result = list()\n",
    "    for sequence in sequences :\n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "# 숫자로 바꿔주는 ~\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [[token_to_id.get(token, unk_id) for token in review] for review in train_tokens]\n",
    "test_ids = [[token_to_id.get(token, unk_id) for token in review] for review in test_tokens]\n",
    "\n",
    "# 패딩\n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataLoader 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hwans\\AppData\\Local\\Temp\\ipykernel_29288\\2142990140.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\hwans\\AppData\\Local\\Temp\\ipykernel_29288\\2142990140.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_ids = torch.tensor(test_ids)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype = torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module) :\n",
    "    def __init__(self, n_vocab, hidden_dim, embedding_dim, n_layers, dropout=0.5, bidirectional=True, model_type=\"lstm\", pretrained_embedding = None) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=n_vocab,\n",
    "                                      embedding_dim = embedding_dim,\n",
    "                                      padding_idx = 0)\n",
    "        \n",
    "        if model_type == \"rnn\" :\n",
    "            self.model = nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers = n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first = True,\n",
    "            )\n",
    "\n",
    "        elif model_type == \"lstm\" :\n",
    "            self.model = nn.LSTM(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size = hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional = bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first = True,\n",
    "            )\n",
    "\n",
    "        if pretrained_embedding is not None :\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype = torch.float32)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embedding=n_vocab,\n",
    "                embedding_dim =embedding_dim,\n",
    "                padding_idx = 0\n",
    "            )\n",
    "\n",
    "        if bidirectional :\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else :\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs) :\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실함수와 최적화 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 674\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    "    ).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6928295493125916\n",
      "Train Loss 500 : 0.7142991265374982\n",
      "Train Loss 1000 : 0.7087508423940523\n",
      "Train Loss 1500 : 0.7067495575354625\n",
      "Train Loss 2000 : 0.7054704243037059\n",
      "Train Loss 2500 : 0.7049807316539097\n",
      "Val Loss : 0.714192312555953, Val Accuracy : 0.4822\n",
      "Train Loss 0 : 0.6821919679641724\n",
      "Train Loss 500 : 0.6989275076670085\n",
      "Train Loss 1000 : 0.7003028777453092\n",
      "Train Loss 1500 : 0.699967139327947\n",
      "Train Loss 2000 : 0.7024520668668904\n",
      "Train Loss 2500 : 0.7033344378999499\n",
      "Val Loss : 0.6891534621723163, Val Accuracy : 0.5456\n",
      "Train Loss 0 : 0.6555588841438293\n",
      "Train Loss 500 : 0.6887910971027649\n",
      "Train Loss 1000 : 0.6814906330673131\n",
      "Train Loss 1500 : 0.6764612208796215\n",
      "Train Loss 2000 : 0.6674276314366764\n",
      "Train Loss 2500 : 0.656788858948875\n",
      "Val Loss : 0.5640580684613115, Val Accuracy : 0.7188\n",
      "Train Loss 0 : 0.5275339484214783\n",
      "Train Loss 500 : 0.5282700491879515\n",
      "Train Loss 1000 : 0.5171164729735711\n",
      "Train Loss 1500 : 0.5065175536312635\n",
      "Train Loss 2000 : 0.4985077992967818\n",
      "Train Loss 2500 : 0.48920897141498165\n",
      "Val Loss : 0.4531024719674747, Val Accuracy : 0.7848\n",
      "Train Loss 0 : 0.5442951917648315\n",
      "Train Loss 500 : 0.3984416989449731\n",
      "Train Loss 1000 : 0.39859390158306707\n",
      "Train Loss 1500 : 0.39927012718553784\n",
      "Train Loss 2000 : 0.39927507572803184\n",
      "Train Loss 2500 : 0.39549811932741286\n",
      "Val Loss : 0.4548237602265117, Val Accuracy : 0.8076\n"
     ]
    }
   ],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval) :\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets) :\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "def test(model, datasets, criterion, device) :\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets) :\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습된 모델로부터 임베딩 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [ 0.00732145 -0.3356088   0.08656503 -0.03306498  0.23212002 -0.29337263\n",
      " -0.09231782 -0.03942776 -0.16306782  0.07997902 -0.02505183 -0.00750695\n",
      "  0.06794622 -0.24104926  0.15246518  0.3419131   0.07585084  0.14669964\n",
      " -0.12381825  0.24512506 -0.01421452  0.25874284 -0.18326129 -0.3010084\n",
      " -0.1685668   0.14352842 -0.12448869  0.282141    0.05883038 -0.27367863\n",
      " -0.07821074  0.18770982 -0.02380344  0.17213023  0.12576792  0.41089207\n",
      "  0.31010926 -0.08500806  0.02139693  0.06602467 -0.09702556  0.27963486\n",
      " -0.04602867 -0.13352565  0.40749404  0.05558844  0.02021302  0.04655577\n",
      "  0.09889083  0.10674627  0.061408   -0.05395599  0.17052408  0.00915676\n",
      "  0.09663107 -0.06783738  0.24049157  0.16517091 -0.13236412 -0.06434153\n",
      "  0.33112514  0.05277485  0.24375214  0.02121567  0.25539294 -0.00287659\n",
      "  0.23966342  0.16198406 -0.20413427 -0.2421456  -0.00624845 -0.1837253\n",
      " -0.42977405 -0.21224083  0.18130228 -0.16075855  0.01933512 -0.03536209\n",
      " -0.04387845  0.40699086 -0.0201764  -0.10097886 -0.22222845  0.45867306\n",
      "  0.19650936  0.30543017  0.34562883 -0.1960105   0.2569692   0.15883519\n",
      "  0.00391834 -0.16748445  0.5496256  -0.19821681  0.05029351  0.14124884\n",
      " -0.4011862  -0.2811857  -0.03421139 -0.20773113 -0.5328476   0.07074025\n",
      " -0.09437876  0.00988229  0.10234187 -0.1308357  -0.11496651 -0.15457876\n",
      "  0.14509346 -0.05731367  0.00520584 -0.34169984  0.04543481 -0.1805773\n",
      " -0.35526094 -0.23041418  0.20694864  0.15216178 -0.25750437  0.0501854\n",
      " -0.21081188 -0.18385148 -0.31503323  0.13850498 -0.29600155 -0.05597539\n",
      " -0.116899   -0.00906207]\n"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix) :\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 학습된 모델로 임베딩 계층 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p289 예제 6-4 + p297 예제 6-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로픔트에서 가상환경 선택하셔서 conda install gensim 해주셔야 해용 \n",
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "tokens = [tokenizer.morphs(review) for review in corpus_df.text]\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences = tokens,\n",
    "    vector_size = 128,\n",
    "    window = 5,\n",
    "    min_count = 1,\n",
    "    sg = 1,\n",
    "    epochs = 3,\n",
    "    max_final_vocab = 10000\n",
    ")\n",
    "word2vec.save(\"../models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
    "\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in id_to_token.items() :\n",
    "    if token not in [\"<pad>\", \"<unk>\"] :\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 학습된 임베딩 계층 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SentenceClassifier(nn.Module) :\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             pretrained_embedding = None\n",
    "#     ) :\n",
    "        \n",
    "#         if pretrained_embedding is not None :\n",
    "#             self.embedding = nn.Embedding.from_pretrained(\n",
    "#                 torch.tensor(pretrained_embedding, dtype = torch.float32)\n",
    "#             )\n",
    "#         else:\n",
    "#             self.embedding = nn.Emedding(\n",
    "#                 num_embedding=n_vocab,\n",
    "#                 embedding_dim =embedding_dim,\n",
    "#                 padding_idx = 0\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 학습된 임베딩을 사용한 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6923500895500183\n",
      "Train Loss 500 : 0.7204446342890848\n",
      "Train Loss 1000 : 0.7105973306950275\n",
      "Train Loss 1500 : 0.7071287550980214\n",
      "Train Loss 2000 : 0.7047542050920207\n",
      "Train Loss 2500 : 0.7038773179578571\n",
      "Val Loss : 0.6534613993602058, Val Accuracy : 0.635\n",
      "Train Loss 0 : 0.8273802399635315\n",
      "Train Loss 500 : 0.6696390861760595\n",
      "Train Loss 1000 : 0.6692885815144538\n",
      "Train Loss 1500 : 0.6530790386757479\n",
      "Train Loss 2000 : 0.617955075501502\n",
      "Train Loss 2500 : 0.5930466720672761\n",
      "Val Loss : 0.45289477763084557, Val Accuracy : 0.784\n",
      "Train Loss 0 : 0.6053240895271301\n",
      "Train Loss 500 : 0.46431599191562856\n",
      "Train Loss 1000 : 0.4564578697665945\n",
      "Train Loss 1500 : 0.45644596096954687\n",
      "Train Loss 2000 : 0.4544312585385396\n",
      "Train Loss 2500 : 0.4513419313934959\n",
      "Val Loss : 0.44205117239929237, Val Accuracy : 0.78\n",
      "Train Loss 0 : 0.4299277067184448\n",
      "Train Loss 500 : 0.42384274699492847\n",
      "Train Loss 1000 : 0.4285786613360509\n",
      "Train Loss 1500 : 0.4420131941007662\n",
      "Train Loss 2000 : 0.4734265914429789\n",
      "Train Loss 2500 : 0.47611490475302076\n",
      "Val Loss : 0.4434415751371902, Val Accuracy : 0.781\n",
      "Train Loss 0 : 0.47058284282684326\n",
      "Train Loss 500 : 0.42765674991403035\n",
      "Train Loss 1000 : 0.42273145093099695\n",
      "Train Loss 1500 : 0.4186077040286957\n",
      "Train Loss 2000 : 0.4175494263569514\n",
      "Train Loss 2500 : 0.41666655935665836\n",
      "Val Loss : 0.4103449945823072, Val Accuracy : 0.81\n"
     ]
    }
   ],
   "source": [
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim = hidden_dim, embedding_dim = embedding_dim,\n",
    "    n_layers=n_layers, pretrained_embedding=init_embeddings\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr = 0.001)\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
