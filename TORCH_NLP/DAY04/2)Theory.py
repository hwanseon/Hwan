# 차원의 저주를 피하고자 임베딩을 사용
# => Word2Vec 임베딩 : 단어 사이의 유사성을 조사해서 적절한 단어를 잘라주는 것 
## => FastText 임베딩, .... 관련 모델이 많음
## -> 속성, 유사성을 잃어버리지 않고 잘라주는 것이 중요 즉, 자기 Data에 맞는 임베딩 기법을 사용해야함 
## 토큰의 수는 단어사전의 수와 상관이 없음 ex) shape(6, 9) 이면 토큰의수 6개와 단어사전의 수 9개는 상관이 없음
## ==> shape(6,9)가 1개의 문장을 구성함
 
## 6개의 토큰이 있어야 1개의 문장이 완성됨 = 시퀀스 or 타임스텍 ==> 한문장을 구성하는 시퀀스의 개수 
# ex) pytorch.org의 코드

# >>> embedding = nn.Embedding(10, 3)
# >>> # a batch of 2 samples of 4 indices each
# >>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])
# >>> embedding(input)
# tensor([[[-0.0251, -1.6902,  0.7172],  위에서 1을 의미
#          [-0.6431,  0.0748,  0.6969],  위에서 2를 의미
#          [ 1.4970,  1.3448, -0.9685],  위에서 4를 의미
#          [-0.3677, -2.7265, -0.1685]], 위에서 5를 의미

#         [[ 1.4970,  1.3448, -0.9685],
#          [ 0.4362, -0.4004,  0.9400],
#          [-0.6431,  0.0748,  0.6969],
#          [ 0.9124, -2.3616,  1.1151]]])

# nn.Embedding(단어사전의 수, 내가 줄일 크기(벡터 차원의 수))

# 모델이름이 Vanilla~는 가장 기본적인 모델이라는 뜻 
# Ice-cream에서 가장 기본적인 맛이 Vanilla라서 거기서 따 온듯

# ---------------------------------------------------------------------------------------------------------
# 쉬는 시간 후

# 임베딩 후 shape(6, 10)에서 BATCH_SIZE가 5이면 -> shape(30, 10)으로 학습 // 그 전에 입력 받은 것을 기억해야하기 떄문 , , ? 
# p304

